{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "tf_TextWorld_RL.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "R7scB-fvXhNr",
    "uyrYMHLb93c1",
    "voGxZYuZT-va",
    "MwHgPKw5WDXG",
    "dKLViRH4gMKY",
    "izOrJDtvs4rO"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dwf4Ml_p49QY"
   },
   "source": [
    "# TeBag-RL - Text-based Adventure Game Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPNo9umRmNlr"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/floriandonhauser/TeBaG-RL/blob/[Linktext](https://)main/tf_TextWorld_RL.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/floriandonhauser/TeBaG-RL\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJQwRDYVlPwi"
   },
   "source": [
    "This is the jupyter notebook for our project for the TUM seminar \"Applied Deep Learning for Natural Language Processing\". We attempted to tackle the difficult combination of NLP with reinforcement learning in form of Deep Q-Learning for text-based adventures games, such as the classic [Zork](https://en.wikipedia.org/wiki/Zork).\n",
    "\n",
    "We are using the [TextWorld](https://github.com/microsoft/TextWorld) environment with a custom PyEnvironment wrapper to use it with TensorFlow. In addition, we utilize custom reward functions and a biased reply buffer accept/reject sampling method. \n",
    "\n",
    "Two agents are tested and trained, one ultiziing an [NNLM](https://tfhub.dev/google/nnlm-en-dim128-with-normalization/2) pre-trained embedding and one with a pre-trained [smallBert](https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1) model. More details can be found in the accompanying materials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BDIAa_mlcXp"
   },
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1O9cgXIypaFV"
   },
   "source": [
    "Installing necessary python packages on either local machine or Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sdhzm1bGhn3U"
   },
   "source": [
    "%%capture\n",
    "!pip install tf-agents\n",
    "!pip install textworld\n",
    "!pip install tensorflow-text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Krs4sqkZhsT-"
   },
   "source": [
    "Google Colab set-up with files in Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hXImSpAcBqWk"
   },
   "source": [
    "%%capture\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "import os\n",
    "PROJECT_PATH = \"/content/drive/MyDrive/TeBaG-RL/\"\n",
    "os.chdir(PROJECT_PATH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UkpwpO2ll8md"
   },
   "source": [
    "***Or alternatively*** importing from GitHub onto (Google Colab) machine:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z9iV-qUKmCpJ"
   },
   "source": [
    "%%capture\n",
    "!git clone https://github.com/floriandonhauser/TeBaG-RL.git"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oP-ZVjV-OzxQ"
   },
   "source": [
    "import os\n",
    "PROJECT_NAME = \"TeBaG-RL/\"\n",
    "os.chdir(PROJECT_NAME)\n",
    "PROJECT_PATH = os.getcwd()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ge9lm5ElgBF"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "85h9Bei-n_al"
   },
   "source": [
    "from resources import DEFAULT_PATHS\n",
    "from tf_train_loop import TWTrainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpLGgA5Xk2NE"
   },
   "source": [
    "## Generate games\n",
    "\n",
    "\n",
    "Generate simple debug game and large dataset of train and eval games."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "svdyaH5irbYK"
   },
   "source": [
    "os.chdir(PROJECT_PATH + \"/scripts/\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "SrYIrqxSWDXB"
   },
   "source": [
    "This cell will create **a single** debug game. This is at least necessary to run anaything."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AMXxZfzGk_fV"
   },
   "source": [
    "%%shell\n",
    "bash ./make_debug_game.sh"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "kf4MPEHJWDXD"
   },
   "source": [
    "**Only** run this if necessary. Depending on system, this process **will take hours**."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tc9fq9C8n9q1"
   },
   "source": [
    "%%capture\n",
    "%%shell\n",
    "bash ./make_allgames.sh"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bX23AdS2riOI"
   },
   "source": [
    "os.chdir(PROJECT_PATH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhIxLnjvWzLh"
   },
   "source": [
    "## Play a game yourself"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X0opeYnDWyU2"
   },
   "source": [
    "os.chdir(\"resources/\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T52VYpKFW_Bl"
   },
   "source": [
    "!tw-play game_th_lvl2_simple.ulx"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JBm0aLP0W-je"
   },
   "source": [
    "os.chdir(PROJECT_PATH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2kCbHw2QeVX"
   },
   "source": [
    "## Test environment\n",
    "\n",
    "We wrote a unit test for the environment creation, as we needed to write a a TensorFlow wrapper for the TextWorld game environment. This also enabled us to add custom rewards and punishments for different scenarios.\n",
    "\n",
    "The environment test function uses TensorFlow buil-in util methods and a random agent. The printed output shows the input command \"Doing: ___\",  the full environment state that we work with (only \"obs\", \"description\" and \"inventory\" are passed to the agent) and the resulting reward."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Tnw8skd2pupn"
   },
   "source": [
    "from tests import test_environment_creation"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HawL9R4V6AYJ"
   },
   "source": [
    "test_environment_creation()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cytRrn9uzM3i"
   },
   "source": [
    "## Run automatic vocab generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlY7R7pcUxfR"
   },
   "source": [
    "We've implemented aa feature in the environment wrapper to check at each time step whether an interactable entity within the TextWorld game is in the current agent object vocabulary. All missing entities are stored and can be appended to the object vocuabulary file at the end of a game cycle.\n",
    "\n",
    "This enables automatic vocabulary generation for a set of training games. The implemented method below utilizes a random agent, however, it needs the full generated training set in the \"Generate Games\" section above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pM9UA8ifzS6f"
   },
   "source": [
    "from environments import run_auto_vocab"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oKGn7KLUzTbl"
   },
   "source": [
    "run_auto_vocab()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1Zx2a9AQoVh"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "pRGei_3AWDXE"
   },
   "source": [
    "Set rewards for training.\n",
    "\n",
    "* **\"win_lose_value\"**: Value to be rewarded/punished for winning/losing the current game\n",
    "* **\"max_loop_pun\"**: Punishmend for having the same state in the buffered set of hashed last states to avoid rewarding loops. (E.g. agent would drop item, go to another room, go back and pick up item while being rewarded for using items and changing the environment.)\n",
    "* **\"change_reward\"**: Reward for changing either inventory (using/taking an item) or or the environment (exploring or opening an object).\n",
    "* **\"useless_act_pun\"**: Punishment for using a non-recoqnizable command (Env-Return: \"I don't understand that.\" or \"I do not see such an object here.\" etc.)\n",
    "* **\"cmd_in_adm\"**: Positive reward if current executed command is in set of admissible commands allowed from the environment. This should encourage environment object linking to commands, even though that command is not the right option.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fXTkK8pKyVX3"
   },
   "source": [
    "REWARD_DICT = {\n",
    "    \"win_lose_value\": 100,\n",
    "    \"max_loop_pun\": 0,\n",
    "    \"change_reward\": 1,\n",
    "    \"useless_act_pun\": 1,\n",
    "    \"cmd_in_adm\": 1,\n",
    "}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5KvonvXAWDXF"
   },
   "source": [
    "### Activate TensorBoard logging"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "TZMwAYvxQ36g"
   },
   "source": [
    "pathdir = DEFAULT_PATHS[\"path_logdir\"]\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir $pathdir"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7scB-fvXhNr"
   },
   "source": [
    "### Overfit on single debug game with each agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mh1VFswxyBw9"
   },
   "source": [
    "Try overfitting on one debug game (correct command \"take x\" will immediately win or lose the game).\n",
    "Depending on whether random agent finds correct WIN command, number of iterations will be enough or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "imbFzQwEJtBt"
   },
   "source": [
    "DEFAULT_HP = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"initial_collect_steps\": 3000,\n",
    "    \"collect_steps_per_iteration\": 1,\n",
    "    \"replay_buffer_max_length\": 100000,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_eval_episodes\": 1,\n",
    "    \"game_gen_buffer\": 25,\n",
    "    \"num_eval_games\": 5,\n",
    "}\n",
    "trainer = TWTrainer(\n",
    "    reward_dict=REWARD_DICT,\n",
    "    hpar=DEFAULT_HP,\n",
    "    debug=False,\n",
    "    biased_buffer=True,\n",
    "    # embedding into fc is default policy\n",
    "    agent_label=\"FCPolicy\",\n",
    ")\n",
    "eval_scores = trainer.train(\n",
    "    num_iterations=2000,\n",
    "    log_interval=100,\n",
    "    eval_interval=100,\n",
    "    game_gen_interval=500,\n",
    "    plot_avg_ret=True,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LL2l3YVFyKoM"
   },
   "source": [
    "DEFAULT_HP = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"initial_collect_steps\": 3000,\n",
    "    \"collect_steps_per_iteration\": 1,\n",
    "    \"replay_buffer_max_length\": 100000,\n",
    "    # large values lead to OOM with bert policy\n",
    "    \"batch_size\": 64,\n",
    "    \"num_eval_episodes\": 1,\n",
    "    \"game_gen_buffer\": 25,\n",
    "    \"num_eval_games\": 5,\n",
    "}\n",
    "trainer = TWTrainer(\n",
    "    reward_dict=REWARD_DICT, \n",
    "    hpar=DEFAULT_HP,\n",
    "    debug=False,\n",
    "    biased_buffer=True,\n",
    "    # embedding into fc is default policy\n",
    "    # agent_label=\"FCPolicy\",\n",
    "    agent_label=\"BertPolicy\",\n",
    ")\n",
    "eval_scores = trainer.train(\n",
    "    num_iterations=2000,\n",
    "    log_interval=100,\n",
    "    eval_interval=100,\n",
    "    game_gen_interval=500,\n",
    "    plot_avg_ret=True,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyrYMHLb93c1"
   },
   "source": [
    "### Train on 10 training games from the same level (level 2) with (simple) Embedding-FC policy agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bLkEje6_95K7"
   },
   "source": [
    "DEFAULT_HP = {\n",
    "    \"learning_rate\": 4.8247e-05,\n",
    "    \"initial_collect_steps\": 30000,\n",
    "    \"collect_steps_per_iteration\": 1,\n",
    "    \"replay_buffer_max_length\": 100000,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_eval_episodes\": 1,\n",
    "    \"game_gen_buffer\": 10,\n",
    "    \"num_eval_games\": 10,\n",
    "}\n",
    "\n",
    "trainer = TWTrainer(\n",
    "    env_dir=\"train_games_lvl2\",\n",
    "    reward_dict=REWARD_DICT,\n",
    "    hpar=DEFAULT_HP,\n",
    "    debug=False,\n",
    "    # !!!!!\n",
    "    biased_buffer=False,\n",
    "    # embedding into fc is default policy\n",
    "    agent_label=\"FCPolicy\",\n",
    "    # agent_label=\"BertPolicy\",\n",
    ")\n",
    "\n",
    "eval_scores = trainer.train(\n",
    "    num_iterations=10000,\n",
    "    log_interval=250,\n",
    "    eval_interval=500,\n",
    "    game_gen_interval=1000000,\n",
    "    rndm_fill_replay=True,\n",
    "    plot_avg_ret=True,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voGxZYuZT-va"
   },
   "source": [
    "### Train on 10 training games from the same level (level 2) with Bert policy agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TdklvFDiimLY"
   },
   "source": [
    "DEFAULT_HP = {\n",
    "    \"learning_rate\": 4.8247e-05,\n",
    "    \"initial_collect_steps\": 30000,\n",
    "    \"collect_steps_per_iteration\": 1,\n",
    "    \"replay_buffer_max_length\": 100000,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_eval_episodes\": 1,\n",
    "    \"game_gen_buffer\": 10,\n",
    "    \"num_eval_games\": 10,\n",
    "    \"num_test_games\": 50,\n",
    "}\n",
    "\n",
    "trainer = TWTrainer(\n",
    "    env_dir=\"train_games_lvl2\",\n",
    "    reward_dict=REWARD_DICT,\n",
    "    hpar=DEFAULT_HP,\n",
    "    debug=False,\n",
    "    biased_buffer=True,\n",
    "    agent_label=\"BertPolicy\",\n",
    ")\n",
    "\n",
    "eval_scores = trainer.train(\n",
    "    num_iterations=10000,\n",
    "    log_interval=250,\n",
    "    eval_interval=250,\n",
    "    game_gen_interval=1000000,\n",
    "    rndm_fill_replay=True,\n",
    "    plot_avg_ret=True,\n",
    "    test_agent=True,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "MwHgPKw5WDXG"
   },
   "source": [
    "### Train on all 1000 training games from the same level (level 2) with NNLM-FC policy agent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s-LGSOqOmx08"
   },
   "source": [
    "DEFAULT_HP = {\n",
    "    \"learning_rate\": 4.8247e-05,\n",
    "    \"initial_collect_steps\": 50000,\n",
    "    \"collect_steps_per_iteration\": 1,\n",
    "    \"replay_buffer_max_length\": 100000,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_eval_episodes\": 1,\n",
    "    \"game_gen_buffer\": 25,\n",
    "    \"num_eval_games\": 10,\n",
    "    \"num_test_games\": 50,\n",
    "}\n",
    "\n",
    "trainer = TWTrainer(\n",
    "    env_dir=\"train_games_lvl2\",\n",
    "    reward_dict=REWARD_DICT,\n",
    "    hpar=DEFAULT_HP,\n",
    "    debug=False,\n",
    "    biased_buffer=True,\n",
    "    agent_label=\"FCPolicy\",\n",
    ")\n",
    "\n",
    "eval_scores = trainer.train(\n",
    "    num_iterations=50000,\n",
    "    log_interval=250,\n",
    "    eval_interval=500,\n",
    "    game_gen_interval=1000,\n",
    "    rndm_fill_replay=True,\n",
    "    plot_avg_ret=True,\n",
    "    test_agent=True,\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKLViRH4gMKY"
   },
   "source": [
    "### Train on all generated  levels with Bert-FC policy agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcR98nKtiw2m"
   },
   "source": [
    "This would be the training loop to utilize multiple levels (with increasing difficulty) of the TreasureHunter type game genereted by TextWorld.\n",
    "\n",
    "They differ in the number of rooms that need to be visited (including dead ends) and the number of other items being available in the environment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "LYD27Tn7_h3u"
   },
   "source": [
    "DEFAULT_HP = {\n",
    "    \"learning_rate\": 4.8247e-05,\n",
    "    \"initial_collect_steps\": 30000,\n",
    "    \"collect_steps_per_iteration\": 1,\n",
    "    \"replay_buffer_max_length\": 100000,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_eval_episodes\": 1,\n",
    "    \"game_gen_buffer\": 10,\n",
    "    \"num_eval_games\": 10,\n",
    "}\n",
    "\n",
    "trainer = TWTrainer(\n",
    "    env_dir=\"train_games_lvl2\",\n",
    "    reward_dict=REWARD_DICT,\n",
    "    hpar=DEFAULT_HP,\n",
    "    debug=False,\n",
    "    biased_buffer=True,\n",
    "    agent_label=\"BertPolicy\",\n",
    ")\n",
    "\n",
    "eval_scores = trainer.train(\n",
    "    num_iterations=10000,\n",
    "    log_interval=250,\n",
    "    eval_interval=500,\n",
    "    game_gen_interval=1000,\n",
    "    rndm_fill_replay=True,\n",
    "    plot_avg_ret=True,\n",
    ")\n",
    "\n",
    "print(f\"Changing to next lvl: 3 \\n\")\n",
    "\n",
    "trainer.change_env_dir(f\"train_games_lvl3\")\n",
    "eval_scores = trainer.train(\n",
    "    num_iterations=10000,\n",
    "    log_interval=250,\n",
    "    eval_interval=500,\n",
    "    game_gen_interval=1000,\n",
    "    continue_training=True,\n",
    "    rndm_fill_replay=True, \n",
    "    plot_avg_ret=True,\n",
    ")\n",
    "\n",
    "print(f\"Changing to next lvl: 4 \\n\")\n",
    "\n",
    "trainer.change_env_dir(f\"train_games_lvl4\")\n",
    "eval_scores = trainer.train(\n",
    "    num_iterations=12000,\n",
    "    log_interval=250,\n",
    "    eval_interval=500,\n",
    "    game_gen_interval=1000,\n",
    "    continue_training=True,\n",
    "    rndm_fill_replay=True,\n",
    "    plot_avg_ret=True,\n",
    ")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izOrJDtvs4rO"
   },
   "source": [
    "## Hyper parameter search\n",
    "\n",
    "Simple hyper parameter search using the Optuna python package. It also comes with very handy visualization tools illustrating correlations and importance between different hypter parameters.\n",
    "\n",
    "To test for stable and good training, the objective value is defined as the average score for the last few iterations for the games in the current buffer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "A4S3R9tSqH-c"
   },
   "source": [
    "%%capture\n",
    "!pip install optuna\n",
    "import optuna\n",
    "import numpy as np\n",
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_slice\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_param_importances"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DQlG-WJQs74O"
   },
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    REWARD_DICT = {\n",
    "        \"win_lose_value\": 100,\n",
    "        \"max_loop_pun\": 0,\n",
    "        \"change_reward\": 1,\n",
    "        \"useless_act_pun\": 1,\n",
    "        \"cmd_in_adm\": 1,\n",
    "    }\n",
    "\n",
    "    DEFAULT_HP = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"lr\", 1e-5, 1e-3,\n",
    "        \"initial_collect_steps\": 30000,\n",
    "        \"collect_steps_per_iteration\": 1,\n",
    "        \"replay_buffer_max_length\": 100000,\n",
    "        #CAREFUL: OOM - Can you handle more than 64?\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 64),\n",
    "        \"num_eval_episodes\": 1,\n",
    "        \"game_gen_buffer\": 10,\n",
    "        \"num_eval_games\": 5,\n",
    "    }\n",
    "\n",
    "    trainer = TWTrainer(\n",
    "        env_dir=\"train_games_lvl2\",\n",
    "        reward_dict=REWARD_DICT,\n",
    "        hpar=DEFAULT_HP,\n",
    "        debug=False,\n",
    "        biased_buffer=True,\n",
    "        agent_label=\"BertPolicy\",\n",
    "    )\n",
    "\n",
    "    eval_scores = trainer.train(\n",
    "        num_iterations=20000,\n",
    "        log_interval=250,\n",
    "        eval_interval=250,\n",
    "        game_gen_interval=1000000,\n",
    "        rndm_fill_replay=True,\n",
    "        plot_avg_ret=True,\n",
    "    )\n",
    "    print(DEFAULT_HP)\n",
    "    print(eval_scores)\n",
    "\n",
    "    return np.mean(eval_scores[1][-5:])\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "print(study.best_trial)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KKFKyzX-ltnR"
   },
   "source": [
    "plot_optimization_history(study)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zSzDHTbQq0Fz"
   },
   "source": [
    "plot_contour(study)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wtkx8PcTq2sm"
   },
   "source": [
    "plot_slice(study)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "67LKUQeHq7t3"
   },
   "source": [
    "plot_param_importances(study)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}